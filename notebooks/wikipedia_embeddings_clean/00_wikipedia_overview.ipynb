{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "\n",
    "1. Download the data dump (a pair of files pair language)\n",
    "2. Process the data dump to a structured format\n",
    "3. Download pageviews data\n",
    "4. Process pageviews data to a structured format per language\n",
    "5. Merge pageviews and wiki data\n",
    "6. Filter to articles we want to keep for embedding\n",
    "    - Use the pageviews data as a proxy for importance/relevance\n",
    "7. Embed the articles\n",
    "8. Store the embeddings\n",
    "\n",
    "\n",
    "### 1 Downloading the data dumps\n",
    "Example pair of files for Icelandic wikipedia:\n",
    "    - iswiki-20240901-pages-articles-multistream.xml.bz2\n",
    "        - Contains the actual wiki articles\n",
    "    - iswiki-20240901-pages-articles-multistream-index.txt.bz2\n",
    "        - Contains the offsets of the articles in the xml file.\n",
    "\n",
    "### 2 Process the data dump to a structured format\n",
    "This is done in the script `src/wikipedia/multiprocess_large_wiki_dump.py`\n",
    "\n",
    "We opt for a duckdb database file per language as the output of this step. \n",
    "\n",
    "A simple .parquet file for a pandas dataframe could be used for small languages, but the larger languages would simply not fit into memory.\n",
    "\n",
    "### 3 Download pageviews data\n",
    "Basically, we get some sort of simple time series data per month. \n",
    "\n",
    " - Each pageviews dump file contains data for a single month.\n",
    " - Each pageviews dump file has the data for all the languages.\n",
    " - Each page appears multiple times in the file (once per day I think)\n",
    "\n",
    "### 4 Process pageviews data to a structured format\n",
    "This is done in the script `src/wikipedia/process_pageviews.py`\n",
    "2 things done here:\n",
    " - Extract the data for the languages we want to keep\n",
    " - Aggregate the data to a monthly level instead of daily\n",
    "\n",
    "### 5 & 6 Merge pageviews and wiki data and do filtering\n",
    "This is done with a notebook. \n",
    "\n",
    "E.g. \n",
    "`notebooks/wikipedia_embeddings_clean/05_english_final_filtering.ipynb`\n",
    "\n",
    "\n",
    "### 7 & 8 Embed the articles and store the embeddings\n",
    "This is done with a notebook.\n",
    "\n",
    "E.g. \n",
    "`notebooks/wikipedia_embeddings_clean/07_english_embed.ipynb`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offical data dumps\n",
    "\n",
    "Wikipedia has an official data dump site: https://dumps.wikimedia.org/\n",
    "\n",
    "A mirror site for e.g. the latest septemper data dump for the Icelandic wikipedia is:  \n",
    "https://mirror.accum.se/mirror/wikimedia.org/dumps/iswiki/20240901/\n",
    "\n",
    "Look for the file: \t\n",
    "- basic data dump: iswiki-20240901-pages-articles.xml.bz2 \n",
    "- with metadata: iswiki-20240901-pages-meta-current.xml.bz2\n",
    "- with multistream: \n",
    "    - iswiki-20240901-pages-articles-multistream.xml.bz2\n",
    "    - iswiki-20240901-pages-articles-multistream-index.txt.bz2\n",
    "\n",
    "**PageViews data**\n",
    " - https://dumps.wikimedia.org/other/pageview_complete/readme.html\n",
    " - https://dumps.wikimedia.org/other/pageviews/\n",
    "    - some doc: https://meta.wikimedia.org/wiki/Research:Page_view\n",
    " - Also pageviews complete:\n",
    "    https://dumps.wikimedia.org/other/pageview_complete/2024/2024-09/\n",
    "\n",
    "\n",
    "## Previous work by others\n",
    "\n",
    " - https://upstash.com/blog/indexing-wikipedia   \n",
    "    - References:\n",
    "    - https://github.com/earwig/mwparserfromhell\n",
    "    - https://huggingface.co/datasets/wikimedia/wikipedia/blob/script/wikipedia.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pageviews as a proxy for importance\n",
    "\n",
    "After looking into ways to estimate the importance of wikipedia pages, it seems that pageviews is the best proxy for importance.  \n",
    "\n",
    "Getting the pageviews data is not as straightforward as I thought it would be.  \n",
    "However, after much digging I found two main ways to get the data. \n",
    " - Data dumps:\n",
    "    - https://dumps.wikimedia.org/other/pageview_complete/\n",
    "        - Need to use the monthly files, and preferably at least a year into the past...\n",
    " - API:\n",
    "    - https://wikimedia.org/api/rest_v1/#/Pageviews_data/get_metrics_pageviews\n",
    "    - https://doc.wikimedia.org/generated-data-platform/aqs/analytics-api/documentation/getting-started.html\n",
    "\n",
    "The downside of the pageviews in general is that it favors older pages.  \n",
    "We may want to do something like x amount of pageviews total for the last year or two + new pages (if we can get that data).\n",
    "Or last edited date. \n",
    "\n",
    "API is probably straight forward, can specify range (e.g. 2 years), so no manual work to combine monthly files.  \n",
    "However for millions of pages this will still take a lot of time, and we need ways to rate limit.  \n",
    "\n",
    "To get started quickly, the data dump is much faster for me. \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
