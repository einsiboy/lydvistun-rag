# RAG Module Specification

Note that the specification below summarizes the main project plan along with the RAG module.  
This is not the main project, this is a submodule of the main project focused on RAG. 

This project is developed separately from the main project to start with only, we will integrate it into the main project later.  
As such the project structure should be something like when creating a python library.  

In this RAG submodule, we will focus on providing the code interface to support all functionality of the RAG module.  
We will not need API endpoints in this submodule, which the main project will handle.  



## 1. Overview

The Retrieval-Augmented Generation (RAG) module is designed to retrieve relevant documents and generate responses for user prompts. It integrates with Pinecone for document retrieval and Together AI's Llama 3.1 model for response generation. This module will be part of a Flask-based web application, primarily handling backend operations while interfacing with the frontend to display RAG responses and source documents.

## 2. Technology Stack

- **Backend**: Flask (with SQLAlchemy for database management)
- **Frontend**: Flask templates, TailwindCSS, DaisyUI, htmx, Alpine.js
- **Database**: SQLite (Development), PostgreSQL (Production via Heroku)
- **Vector Database**: Pinecone for vector storage and retrieval
- **Model Inference**: Together AI (Llama 3.1 for generating responses)
- **Hosting**: Heroku

---

## 3. RAG Workflow

### 3.1 Document Retrieval

- Retrieve relevant documents based on a given user prompt.
- Documents are stored and retrieved as vectors in Pinecone.
- Use vector similarity search to return the most relevant documents.

### 3.2 Response Generation

- Generate two categories of responses:
  - **Non-RAG Responses**: Generated based solely on the user prompt.
  - **RAG Responses**: Generated by augmenting the prompt with relevant documents retrieved from Pinecone.
  
Together AI's Llama 3.1 model will handle the response generation for both categories.

---

## 4. Response Types

For each user prompt, the system generates the following types of responses:

### 4.1 Non-RAG Responses

- Short response
- Longer response
- Informative/serious response
- Funny response

### 4.2 RAG Responses

- Short response
- Longer response
- Informative/serious response
- Funny response

Both types of responses (RAG and non-RAG) will be generated for each prompt to allow user comparison.

---

## 5. Source Document Display

For responses generated using the RAG approach, the following features will be available in the UI:

- **Collapsible Source Document Section**: Retrieved documents will be displayed in a collapsible section to provide transparency on the sources used to generate the response.
- **Highlighted Documents**: Documents that contain the most relevant or correct information will be highlighted for easy user reference.

---

## 6. User Interaction

### 6.1 Evaluation and Ranking

- Users will evaluate and rank both RAG and non-RAG responses using a Likert scale (1 to 5) for factors such as quality and creativity.
- Users will be able to compare responses with and without retrieval to identify the best response.

### 6.2 Response Improvement

- Users will have the option to rewrite or improve the generated responses, particularly those created with RAG.
- These user-improved responses will be stored for future evaluation and analysis.

### 6.3 Extended Conversation

- Users can flag responses to initiate extended conversations.
- An entire conversation context, including previous responses and retrieved documents, can be used to generate new synthetic responses.

---

## 7. Data Management

### 7.1 Document Insertion into Pinecone

- The module will support the insertion of new documents into Pinecone.
- Documents will be converted into vector embeddings using a pre-trained embedding model (e.g., OpenAI embeddings) and stored in Pinecone with associated metadata such as title, source, and language.

### 7.2 Vector Search and Storage

- The system will perform vector similarity searches in Pinecone to retrieve relevant documents based on the user prompt.
- Retrieved documents will include metadata to allow contextually informed response generation.

---

## 8. Performance and Scalability

- **Asynchronous Requests**: Document retrieval (Pinecone) and response generation (Together AI) will be handled asynchronously to support concurrent users.
- **Caching**: Frequently retrieved documents and responses may be cached to reduce the number of API calls and improve performance.
- **Database Optimization**: The system will use optimized queries and indexes to ensure efficient data retrieval and response handling.

---

## 9. Security Considerations

- **API Security**: Secure API calls will be implemented to interact with Pinecone and Together AI, using API keys to protect against unauthorized access.
- **Data Privacy**: Ensure that any retrieved documents containing personally identifiable information (PII) are handled appropriately to avoid privacy violations.

---

## 10. User Interface Requirements

### 10.1 Collapsible Source Display

- The UI should support a collapsible section for showing retrieved source documents used in RAG responses. Users should be able to expand or collapse this section to view or hide the documents.

### 10.2 Response Ranking

- Provide an intuitive interface for users to rank and evaluate both RAG and non-RAG responses. Use a Likert scale for evaluation and offer ranking options based on overall quality.

### 10.3 Response Improvement UI

- Allow users to easily rewrite or improve responses using a text input area.
- Save the improved responses for further evaluation and comparison with the original responses.

---

## 12. Conclusion

The RAG module is a Python-based, mostly standalone system for document retrieval and response generation. It integrates with Pinecone for vector storage and retrieval and Together AI for response generation. The module will interface with the frontend to support user evaluation, response ranking, and improvement. Its scalable and secure design ensures reliable performance in a web-based environment.
